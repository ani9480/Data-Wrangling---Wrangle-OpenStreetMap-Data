{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle OpenStreetMap Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "* Map Area: Mumbai, Maharashtra, India\n",
    "\n",
    "* Dataset: This data contains information about the city Mumbai also called the 'city of dreams' since the Indian Movie industry     is located in this city also known as Bollywood. Every year thousands of people come here to either get in the movie industry or music industry. Besides all this there are many tourists spots and old monuments.\n",
    "I have chosen this dataset since i live in a city nearby called Pune. I wanted to explore what kind of information the Open Street Map data contains about Mumbai since this is an important place so i wanted to contribute to the data by removing any errors if any."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems Encountered in the Map\n",
    "#### 1. Identifying the problems in the extract\n",
    "\n",
    "Size of the dataset : 406 MB\n",
    "Size of sample extract I have created by the code which was given: 41.1 MB\n",
    "\n",
    "\n",
    "I used two techniques for identifying any problems in the sample dataset.\n",
    "* Analyzing the sample osm data itself by opening in an editor.\n",
    "* Modifying the \"audit.py\" and \"process_osm.py\" which created the sample osm JSON file and after wards analyzing that data set to search or identify errors if any. \n",
    "* Again repeat the steps above.\n",
    "\n",
    "#### 2.Problems faced\n",
    "\n",
    "After doing the analysis I found four main problems. These are:\n",
    "\n",
    "#### a. Over-abbreviated Street Names and Street names in regional language. \n",
    "\n",
    "For correcting this I mapped the inconsistent data to correct data. A sample example is shown below.\n",
    "* \"Vasai Station Rd\" : \"Vasai Station Road\"\n",
    "* \"Yashavant Nagar Rd\" : \"Yashavant Nagar Road\"\n",
    "* \"mankoli naka\" : \"mankoli Toll\"\n",
    "\n",
    "There were also inconsistent street name which made no sense at all and i was not sure what to do so i took them in a lsit and removed those nodes or ways from my analysis.\n",
    "\n",
    "Invalid street names\n",
    " = [\"govandi\" , \"World\" , \"compound\" ,\"Powai\", \"Multiplex\", \"Gokuldham\"]\n",
    "\n",
    "#### b. Incorrect Phone number format. \n",
    "\n",
    "The data has several numbers which are in the wrong format, some of them are given below.\n",
    "\n",
    "*  '022 2643 5361'\n",
    "*  '+91 22 2265 4194 / 2263 0393'\n",
    "*  '+98 22 65 28 52 84'\n",
    "*  '8108957786'\n",
    "*  '(91)-9972526110'\n",
    "\n",
    "\n",
    "First of all the length of a phone number should be 10 and it should be preceded by our country code '+91' for example\n",
    "\n",
    "* \"(91)-9972526110\" to \"+919972526110\"\n",
    "* \"022 2643 5361\" to \"+912226435361\"\n",
    "\n",
    "#### c. Incorrect cuisine and bank names \n",
    "\n",
    "After auditing the data and creating the JSON file. WHen i started to query the data. I found out that the names of these attributes are inconsistent. SO like street name data I mapped them to thier correct description. Below are some sample  mappings I have done.\n",
    "\n",
    "Cuisine Mapping\n",
    "\n",
    "* \"burger\" : \"American\",\n",
    "* \"pizza\" : \"Italian\",\n",
    "* \"coffee\" : \"Italian\",\n",
    "* \"regional\": \"Maharashtrian\"\n",
    "\n",
    "Bank Names Mapping\n",
    "\n",
    "* \"Greater\" : \"Greater Bombay Co-operative Bank\",\n",
    "* \"Union\" : \"Union Bank of India \",\n",
    "* \"State\" : \"State Bank of India\",\n",
    "* \"SBI\" : \"State Bank of India\",\n",
    "\n",
    "\n",
    "#### d. Name field not there in some of the documents\n",
    "\n",
    "While analyzing the data I found that some documents did not have the name field in it, instead their name was given in different fields like \"operator\" or \"brand. So I modified the \"process_osm.py\" to get the name field populated in such cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auditing the data and filtering out inconsistent information\n",
    "\n",
    "For this purpose i have written the code below to implement the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.sample_creater.py\n",
    "\n",
    "Creates a sample extract of the main OSM file so that it is easy to work with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"C:/New folder/mumbai_india.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"C:/New folder/sample_mumbai.osm\"\n",
    "\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write(bytes('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n', 'UTF-8'))\n",
    "    output.write(bytes('<osm>\\n  ', 'UTF-8'))\n",
    "\n",
    "    # Write every 10th top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % 10 == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write(bytes('</osm>', 'UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. audit.py\n",
    "\n",
    "This code audits the data extract, finds the problems and updates them with proper information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "OSMFILE = \"sample_mumbai.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "unwanted = [\"govandi\" , \"World\" , \"compound\" ,\"Powai\", \"Multiplex\", \"Gokuldham\"]\n",
    "\n",
    "# UPDATE THIS VARIABLE\n",
    "\n",
    "#creating mapping for street, cuisines and bank names\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Rd.\" : \"Road\",\n",
    "            \"Rd\" : \"Road\",\n",
    "            \"Ave\" : \"Avenue\",\n",
    "            \"stn\" : \"Station\",\n",
    "            \"naka\" : \"Toll\",\n",
    "            \"Naka\" : \"Toll\",\n",
    "            \"Restauran\" : \"Restaurant\"\n",
    "            }\n",
    "\n",
    "cuisine_mapping = {\"burger\" : \"American\",\n",
    "                   \"pizza\" : \"Italian\",\n",
    "                   \"coffee\" : \"Italian\",\n",
    "                   \"regional\": \"Maharashtrian\",\n",
    "                   \"cake\" : \"Pastry\",\n",
    "                   \"vegetarian\":\"Indian\",\n",
    "                   \"coffee_shop\" : \"Italian\",\n",
    "                   \"bread amlet\" : \"Indian\",\n",
    "                   \"all_types_of_food\": \"Indian\",\n",
    "                   \"indian_aagri\": \"Indian\",\n",
    "                   \"international\" : \"continental\"\n",
    "    \n",
    "                   }\n",
    "\n",
    "\n",
    "\n",
    "bank_mapping = {\"Greater\" : \"Greater Bombay Co-operative Bank\",\n",
    "                \"Union\" : \"Union Bank of India \",\n",
    "                \"State\" : \"State Bank of India\",\n",
    "                \"Dena\" : \"Dena Bank\",\n",
    "                \"HDFC\" : \"HDFC Bank\",\n",
    "                \"Kotak\" : \"Kotak Mahindra Bank\",\n",
    "                \"Central\": \"Central Bank of India\",\n",
    "                \"Karnataka\": \"Karnataka Bank\",\n",
    "                \"HSBC\" : \"HSBC Bank\",\n",
    "                \"SBI\" : \"State Bank of India\",\n",
    "                \"Raheja\" : \"Raheja Classique Bank\",\n",
    "                \"Punjab\" : \"Punjab National Bank\",\n",
    "                \"AXIS\" : \"AXIS Bank\"\n",
    "               }\n",
    "\n",
    "#creating a procedure audit_street_type for cleaning the street names\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)            # using regex to get the street name \n",
    "   \n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected and not street_type.isdigit() and street_type not in unwanted:\n",
    "            if ',' not in street_type:\n",
    "                if street_type == 'Maharashtra':\n",
    "                    street_types[\"Road\"].add(street_name)\n",
    "                else:\n",
    "                    street_types[street_type].add(street_name)\n",
    "            else:\n",
    "                pos_type = street_type.find(',')\n",
    "                pos_name = street_name.find(',')\n",
    "                \n",
    "                street_type = street_type[:pos_type]\n",
    "                street_name = street_name[:pos_name]\n",
    "                #print (street_name)\n",
    "                \n",
    "                street_types[street_type].add(street_name)\n",
    "    \n",
    "\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    osm_file = open(osmfile, encoding=\"utf8\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "\n",
    "#defining procedures for cleaning and updating name field, telephone field, cuisines and bank names field.\n",
    "\n",
    "def update_name(name, mapping):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    last_word = street_type_re.search(name)\n",
    "    last_word = (last_word.group(0))\n",
    "    if (last_word) in (mapping.keys()):\n",
    "        \n",
    "        name = name.replace(last_word, mapping[last_word])\n",
    "        #print(name)\n",
    "    return name\n",
    "\n",
    "def update_telephone_no(nos):\n",
    "    mod_num=''\n",
    "    if '/' in nos:\n",
    "                 \n",
    "        splittednmbers=re.split('/',nos)\n",
    "        #print(splittednmbers)\n",
    "        value=[]\n",
    "        for c in splittednmbers:\n",
    "            b=''\n",
    "            \n",
    "            c=re.split('[^0-9]',c)\n",
    "            for i in c:\n",
    "                b = b + i\n",
    "            if '91' not in b[0:2]:    \n",
    "                value.append('+9122' + b)\n",
    "            else:\n",
    "                value.append('+' + b)\n",
    "        return(value)\n",
    "      \n",
    "    else:\n",
    "        nos= re.split('[ ()-]',nos)\n",
    "        for num in nos:\n",
    "            mod_num = mod_num + num\n",
    "            \n",
    "        \n",
    "        length = len(mod_num)\n",
    "        rem = length - 10\n",
    "        mod_num =  '+91' +  mod_num[rem:]\n",
    "        return (mod_num)\n",
    "        \n",
    "def update_food(items):\n",
    "    \n",
    "    \n",
    "    if ';' in items:\n",
    "        new_cuisine = re.split('[;]',items)\n",
    "        if 'asian' in new_cuisine or 'french' in new_cuisine:\n",
    "            return('continental')\n",
    "        elif 'indian' in new_cuisine:\n",
    "            return('indian')\n",
    "        elif 'american' in new_cuisine:\n",
    "            return('american')\n",
    "            \n",
    "    elif items in cuisine_mapping:\n",
    "        return(cuisine_mapping[items])\n",
    "    else:\n",
    "        return(items)\n",
    "    \n",
    "    \n",
    "def update_bank_name(name):\n",
    "    pos = name.find(\" \",1)\n",
    "    #print(pos)\n",
    "    if pos != -1:\n",
    "        first_word = name[:pos]\n",
    "        if first_word in bank_mapping:\n",
    "            return bank_mapping[first_word]\n",
    "        else:\n",
    "            return name\n",
    "    elif name in bank_mapping:\n",
    "        return bank_mapping[name]\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_types = audit(OSMFILE)   # calling the audit() prcedure to create the updated name set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. process_osm.py\n",
    "\n",
    "This file is very important since here I am creating the JSON type document for MongoDB plus it also does some small data cleaning also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "from pprint import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "    \n",
    "\n",
    "##the procedure below helps in filtering the fields with proper naming conventions and data. it also creates the JSON object \n",
    "##structure for each node and way which we need for our analysis purpose\n",
    "    \n",
    "def shape_element(element):\n",
    "    node = {}\n",
    "    address={}\n",
    "    if element.tag == \"node\" or element.tag == \"way\":\n",
    "        #print element.attrib\n",
    "        #YOUR CODE HERE\n",
    "        node[\"type\"] = element.tag\n",
    "        node[\"created\"] = {}\n",
    "        node[\"pos\"] =[ None , None]\n",
    "        \n",
    "        for key,val in element.attrib.items():\n",
    "            \n",
    "            if key in CREATED:\n",
    "                \n",
    "                node[\"created\"].update({key:val})\n",
    "            elif key == 'lat':\n",
    "                node[\"pos\"][0] = float(element.get(key))\n",
    "            elif key == 'lon':\n",
    "                node[\"pos\"][1] = float(element.get(key))\n",
    "            else:\n",
    "                node[key] = element.get(key)\n",
    "                \n",
    "                \n",
    "        \n",
    "        a= []\n",
    "        \n",
    "        for i in element:\n",
    "            \n",
    "            if i.tag == 'nd':\n",
    "                \n",
    "                for refs,val in i.attrib.items():\n",
    "                    #print(val)\n",
    "                    a.append(val)\n",
    "            node[\"node_refs\"] = a\n",
    "            \n",
    "            if i.tag == 'tag':\n",
    "                \n",
    "                node[\"address\"] = {}\n",
    "                empty_dict = {}\n",
    "               \n",
    "                for loc,val in i.attrib.items():\n",
    "                    \n",
    "                    if val.startswith(\"addr:\") and lower_colon.match(val):\n",
    "                                \n",
    "                                key, value = val.split(\":\")\n",
    "                                #empty_dict = dict.fromkeys([value])\n",
    "                                new_val = update_name(i.attrib['v'], mapping)\n",
    "                                \n",
    "                                if value == 'city':\n",
    "                                    if new_val.lower() != 'mumbai':\n",
    "                                        address[value] = 'Mumbai'\n",
    "                                        address[\"location\"] = new_val\n",
    "                                else:\n",
    "                                    address[value] = new_val\n",
    "                                \n",
    "                    elif \"addr:\" not in val and lower.match(val) and loc != 'v':\n",
    "                        if val.lower() == \"phone\" or val.lower() == 'fax':\n",
    "                            node[val] = update_telephone_no(i.attrib['v'])\n",
    "                            \n",
    "                        elif val.lower() == \"cuisine\":\n",
    "                            node[val] = update_food(i.attrib['v'])\n",
    "                        elif val.lower() == \"amenity\":\n",
    "                            if 'atm' in i.attrib['v'].lower():\n",
    "                                node[val] = i.attrib['v']\n",
    "                                node[\"atm\"] = \"yes\"\n",
    "                            else:\n",
    "                                node[val] = i.attrib['v']\n",
    "                        else:\n",
    "                            node[val] = i.attrib['v']\n",
    "                    \n",
    "                    elif \"name\" in val:\n",
    "                        \n",
    "                        if len(i.attrib['v']) != -1:\n",
    "                            #print (i.attrib['v'])\n",
    "                            new_name = update_name(i.attrib['v'], mapping)\n",
    "                            node[val] = new_name\n",
    "                            \n",
    "                    elif problemchars.match(val):\n",
    "                        pass\n",
    "                                \n",
    "                node[\"address\"] = address   \n",
    "                \n",
    "        if \"operator\" in node and \"name\" not in node:\n",
    "            node[\"name\"] = node[\"operator\"]\n",
    "        \n",
    "        if \"brand\" in node and \"name\" not in node:\n",
    "            node[\"name\"] = node[\"brand\"]\n",
    "            \n",
    "        if \"name\" in node:\n",
    "            if \"atm\" in node:\n",
    "                #print(node[\"name\"])\n",
    "                node[\"name\"] = update_bank_name(node[\"name\"])  \n",
    "                \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        return None\n",
    "    return node\n",
    "\n",
    "def process_map(file_in, pretty = False):\n",
    "    # You do not need to change this file\n",
    "    file_out = \"{0}.json\".format(file_in)\n",
    "    data = []\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = process_map('sample_mumbai.osm', True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data to MongoDB\n",
    "\n",
    "For this purpose I am using the JSON file created above and running this command in the windows command prompt\n",
    "\n",
    "#### mongoimport --db openStreetMapData --collection Mumbai --type json --file sample_mumbai.osm.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Connecting to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "\n",
    "client = MongoClient('localhost:27017')\n",
    "db = client.openStreetMapData            #database name is openStreetMapData\n",
    "coll = db.Mumbai                        # collection name is Mumbai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Total Documents (nodes and ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232900\n"
     ]
    }
   ],
   "source": [
    "result = coll.find().count()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Total number of Nodes in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204562"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll.find( { \"type\": \"node\" } ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Total number of Ways in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28298"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll.find( { \"type\": \"way\" } ).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique contributers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "873"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(coll.distinct( \"created.user\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User ID who has contributed the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'PlaneMad', 'count': 7852}\n"
     ]
    }
   ],
   "source": [
    "def pipeline():    \n",
    "    pipeline = [{\"$match\" : {\"created.user\" : {\"$exists\" : True , \"$ne\" : None }}},                            \n",
    "                \n",
    "                {\"$group\": {\"_id\" : \"$created.user\",                \n",
    "                            \"count\" : {\"$sum\" :1}}},\n",
    "                                                \n",
    "                {\"$sort\" : {\"count\" : -1}},\n",
    "                \n",
    "                {\"$limit\" : 1}\n",
    "         \n",
    "                ]\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "output = pipeline()    \n",
    "result = coll.aggregate(output)\n",
    "for i in result:\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of top 10 amenities in the city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'place_of_worship', 'count': 53}\n",
      "{'_id': 'restaurant', 'count': 41}\n",
      "{'_id': 'bank', 'count': 30}\n",
      "{'_id': 'school', 'count': 29}\n",
      "{'_id': 'cafe', 'count': 19}\n",
      "{'_id': 'hospital', 'count': 18}\n",
      "{'_id': 'atm', 'count': 17}\n",
      "{'_id': 'fast_food', 'count': 17}\n",
      "{'_id': 'parking', 'count': 15}\n",
      "{'_id': 'fuel', 'count': 14}\n"
     ]
    }
   ],
   "source": [
    "def pipeline():    \n",
    "    pipeline = [{\"$match\" : {\"amenity\" : {\"$exists\" : True , \"$ne\" : None }}},                            \n",
    "                \n",
    "                {\"$group\": {\"_id\" : \"$amenity\",                \n",
    "                            \"count\" : {\"$sum\" :1}}},\n",
    "                                                \n",
    "                {\"$sort\" : {\"count\" : -1}},\n",
    "                \n",
    "                {\"$limit\" : 10}\n",
    "         \n",
    "                ]\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "output = pipeline()    \n",
    "result = coll.aggregate(output)\n",
    "for i in result:\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since India is a country which has many religions there must be many places of worship for all those religions. So i guess the output is justifying it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many fuel stations are there in the city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll.find( { \"amenity\" : \"fuel\" }).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of atms in sample location of Mumbai I have selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "result = coll.find({\"atm\" : \"yes\"}).count()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which bank has the most ATMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'HDFC Bank', 'number_of_atms': 5}\n",
      "{'_id': 'State Bank of India', 'number_of_atms': 5}\n",
      "{'_id': 'Union Bank of India ', 'number_of_atms': 3}\n",
      "{'_id': 'ICICI Bank', 'number_of_atms': 2}\n",
      "{'_id': 'Kotak Mahindra Bank', 'number_of_atms': 2}\n",
      "{'_id': 'Raheja Classique Bank', 'number_of_atms': 1}\n",
      "{'_id': 'Karnataka Bank', 'number_of_atms': 1}\n",
      "{'_id': 'private', 'number_of_atms': 1}\n",
      "{'_id': 'Corporation Bank', 'number_of_atms': 1}\n",
      "{'_id': 'THE HINDUSTHAN CO-OP.BANK LTD MUMBAI', 'number_of_atms': 1}\n",
      "{'_id': 'Dena Bank', 'number_of_atms': 1}\n",
      "{'_id': 'AXIS Bank', 'number_of_atms': 1}\n",
      "{'_id': 'Punjab National Bank', 'number_of_atms': 1}\n",
      "{'_id': 'IDBI', 'number_of_atms': 1}\n",
      "{'_id': 'Bank of India', 'number_of_atms': 1}\n",
      "{'_id': 'HSBC Bank', 'number_of_atms': 1}\n",
      "{'_id': 'Greater Bombay Co-operative Bank', 'number_of_atms': 1}\n",
      "{'_id': 'DBS Bank', 'number_of_atms': 1}\n"
     ]
    }
   ],
   "source": [
    "def pipeline():    \n",
    "    pipeline = [{\"$match\" : {\"name\" : {\"$exists\" : True , \"$ne\" : None },\n",
    "                            \"atm\" : \"yes\"}},\n",
    "                \n",
    "                {\"$group\": {\"_id\" : \"$name\",                \n",
    "                            \"count\" : {\"$sum\" :1}}},\n",
    "                \n",
    "                {\"$project\": {\"name\" : \"$name\",\n",
    "                             \"number_of_atms\": \"$count\"}},\n",
    "                \n",
    "                {\"$sort\" : {\"number_of_atms\" : -1}}\n",
    "         \n",
    "                ]\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "output = pipeline()    \n",
    "result = coll.aggregate(output)\n",
    "for i in result:\n",
    "    pprint(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that HDFC bank and State Bank of India has the most number of ATMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Most popular bank in the region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'State Bank of India', 'popularity_score': 4}\n",
      "{'_id': 'HDFC Bank', 'popularity_score': 4}\n",
      "{'_id': 'Kotak Mahindra Bank', 'popularity_score': 3}\n",
      "{'_id': 'Union Bank of India ', 'popularity_score': 3}\n",
      "{'_id': 'ICICI Bank', 'popularity_score': 2}\n",
      "{'_id': 'Union Bank of India', 'popularity_score': 1}\n",
      "{'_id': 'Karnataka Bank', 'popularity_score': 1}\n",
      "{'_id': 'Greater Bombay Co-operative Bank', 'popularity_score': 1}\n",
      "{'_id': 'THE HINDUSTHAN CO-OP.BANK LTD MUMBAI', 'popularity_score': 1}\n",
      "{'_id': 'Dena Bank', 'popularity_score': 1}\n",
      "{'_id': 'American Express Bank', 'popularity_score': 1}\n",
      "{'_id': 'ANZ Grindlays', 'popularity_score': 1}\n",
      "{'_id': 'Central Bank', 'popularity_score': 1}\n",
      "{'_id': 'Bank of India', 'popularity_score': 1}\n",
      "{'_id': 'ABN Amro', 'popularity_score': 1}\n",
      "{'_id': 'Punjab National Bank', 'popularity_score': 1}\n",
      "{'_id': 'Greater Bank', 'popularity_score': 1}\n"
     ]
    }
   ],
   "source": [
    "def pipeline():    \n",
    "    pipeline = [{\"$match\" : {\"amenity\" :{ \"$eq\" : \"bank\", \"$ne\" : None},\n",
    "                            \"name\" : {\"$exists\" : True , \"$ne\" : None }}},\n",
    "                \n",
    "                {\"$group\": {\"_id\" : \"$name\",\n",
    "                \n",
    "                            \"count\" : {\"$sum\" :1}}},\n",
    "                \n",
    "                {\"$project\": {\"name\" : \"$name\",\n",
    "                             \"popularity_score\": \"$count\"}},\n",
    "                \n",
    "                {\"$sort\" : {\"popularity_score\" : -1}}\n",
    "         \n",
    "                ]\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "output = pipeline()    \n",
    "result = coll.aggregate(output)\n",
    "for i in result:\n",
    "    pprint(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here also I have got the result as expected most popular banks are HDFC Bank and State Bank of India"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Listing out the popular cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'indian', 'count': 15}\n",
      "{'_id': 'italian', 'count': 11}\n",
      "{'_id': 'maharashtrian', 'count': 4}\n",
      "{'_id': 'american', 'count': 2}\n",
      "{'_id': 'continental', 'count': 2}\n",
      "{'_id': 'chinese', 'count': 2}\n",
      "{'_id': 'seafood', 'count': 1}\n"
     ]
    }
   ],
   "source": [
    "def pipeline():    \n",
    "    pipeline = [{\"$match\" : {\"cuisine\" :{ \"$exists\" : True, \"$ne\" : None}}},\n",
    "                \n",
    "                {\"$project\": { \"cuisine\": { \"$toLower\": \"$cuisine\" }}},\n",
    "                            \n",
    "                \n",
    "                {\"$group\": {\"_id\" : \"$cuisine\",\n",
    "                \n",
    "                            \"count\" : {\"$sum\" :1}}},\n",
    "                      \n",
    "                \n",
    "                {\"$sort\" : {\"count\" : -1}}\n",
    "         \n",
    "                ]\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "output = pipeline()    \n",
    "result = coll.aggregate(output)\n",
    "for i in result:\n",
    "    pprint(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that Indian and Italian cuisine are more preferred in Mumbai. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Additional Improvements in Dataset\n",
    "\n",
    "Earlier while querying the data, I saw there were numerous documents which did not have any name field , any address or any proper attribute which would highlight what the data is about. In MongoDB I have inserted those kind of documents, since it is not feasible to cleanse such kind of data. There can be thousands of such datasets where you don't have any proper highlighting key. So it be a tedious job to cleaning out all these uninformative elements. But we need to find a way to ignore these kind of data since these takes up much of the space. I hope to contribute to this by searching the internet and finding new ways to do this.\n",
    "\n",
    "So now I find the number of documents that do not include name fields in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231167"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coll.find({\"name\" : {\"$exists\" : False }}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SO earlier we got total number of documents as 232900. SO we can see that only 1733 documents are perfectly formatted or we can say. About 99% of the data is filled with uninformative elements. So we need to find a way to remove them or they are taking up too many space. And this was just a sample what if there is a huge dataset. We need to find clever way to cleanse the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The Mumbai data set was very messy with too much of bad data. I tried to cover as much of data wrangling and data cleaning as possible for our analysis and for this project. And for Open Street Map I think they should modify or update their process of taking inputs such that when a user is updating any location it should be mandatory to give a key which defines the location or else we would get bad data out of it. I hope the analysis covers some interesting insights about the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "##### 1. https://wiki.openstreetmap.org/wiki/OSM_XML\n",
    "##### 2. https://www.openstreetmap.org/#map=5/51.500/-0.100\n",
    "##### 3. https://mapzen.com/data/metro-extracts/\n",
    "##### 4. https://docs.mongodb.com/manual/\n",
    "##### 5. https://stackoverflow.com/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
